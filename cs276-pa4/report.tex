\documentclass[10pt,twocolumn]{article}
\usepackage{fullpage,enumerate,amsmath,amssymb,graphicx,setspace,epstopdf,float,multirow}

\begin{document}
\title{CS 276 Programming Assignment 4 Project Report}
\author{Jiaji Hu, Xuening Liu}
\date{}
\maketitle

\section*{Task 1: Pointwise Approach}
In this part, we use linear regression to give a score to each query-document pair. The query-document pairs are represented by a five-dimensional vector of tf-idf scores.

Since we cannot start adding new features at this point, and there are no parameters to tune for linear regression, there is not much for us to design ourselves in this task. Here, we focus on two design choices:

Firstly, we determined whether or not to use sublinear scaling for term frequencies. By using sublinear scaling, we design the system so that high frequencies of a single term have a smaller impact on the score. Conceptually this makes sense, because having a query term appear 10 times is probably better than having it appear 1 time, but it probably shouldn't be 10 times as good. With sublinear scaling, our linear regression model achieved a high score, so we used sublinear scaling.

Secondly, we tried applying length normalization to each document. The idea is that since longer documents are more likely to have more query terms appear, we should penalize them for their length. Furthermore, we can smooth the document lengths for better performance. It turned out that document length normalization did not help performance on this task. However, length normalization did help in the following tasks, as we will discuss later.

Finally, our performance for task 1:
\begin{table}[H]
\centering
\begin{tabular}{|c|c|}
\hline
Dataset & NDCG/iNDCG score \\\hline
training & 0.8749\\\hline
dev & 0.8429\\\hline
\end{tabular}
\end{table}

\section*{Task 2: Pairwise Approach}
The pairwise approach is totally different from the pointwise approach, in that we do not directly try to predict a relevance score for a query-document pair, but train our classifier to compare the relevance of different query-document pairs.

Similar to task 1, sublinear scaling of term frequencies and document length normalization are two things we can do to help the performance of the ranking system. In this task, both sublinear scaling and length normalization gave the NDCG score a considerable boost, so we do both these operations in our final system.

Standardization for the input for the SVM is necessary for fast runtime and good performance. In the assignment guidelines, we were instructed to do standardization \underline{before} we took the difference of vectors. Also, we were instructed to do standardization independently on the training and test set. We felt that these might not be the correct practice, but rather shortcuts to make the assignment simpler. Therefore, we implemented both versions of standardization to see if they lead to any difference in performance.

Firstly, since the actual vectors that get sent into the SVM for training are the vectors after we take the difference, it makes more sense to do standardization on that dataset. Secondly, it is a good idea to do the same preprocessing to training and test data, so we should not do separate standardization for training and test. In our second implementation, we implemented our own standardization method that remembers the mean and standard deviation of the training data, and apply the same filter to the test data as the training data.

In practice, both the standardization recommended in the guidelines and our own standardization give good results. It makes sense that the shortcuts work. For the first shortcut, we know that we don't need the input data to exactly have zero mean and unit variance for SVM to work well. Standardization before taking the difference gives a distribution that is good enough. For the second shortcut, we rely on the fact that the training and test data have very similar distributions, so doing standardization separately yields very similar results as doing the same transformation on the two datasets.

To tune the SVM parameters, we used grid search on parameter \texttt{C} for linear kernel SVM, and parameters $C$ and $\gamma$ for RBF SVM. We searched on the following range: $C=\{2^{-3},2^{-2},\dots,2^{3}\}$, and $\gamma=\{2^{-7},2^{-6},\dots,2^{-1}\}$. We noticed that with large $C$ and large $\gamma$, the training time goes up considerably. Also, the performance of the linear SVM does not vary a lot with $C$, and the liner SVM performs very well even under default parameters. For the RBF SVM, we got some good performance with $C=\{2^{2},2^{3}\}$ and $\gamma=\{2^{-4},2^{-5}\}$, though there is a lot of variance in the results.

Comparing the linear and RBF kernel SVMs, both give good performance. The RBF SVM got a slightly higher top NDCG score after grid search, but not by much. On the other hand, the linear SVM is handy because it is generally faster to train and is not as sensitive to parameter adjustment.

Finally, our performance for task 2:
\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
Dataset & Linear SVM & RBF SVM \\\hline
training & 0.8627 & 0.8637 \\\hline
dev & 0.8472 & 0.8479\\\hline
\end{tabular}
\end{table}

\section*{Task 3: More Features and \\Error Analysis}
In this task, we add extra features to the model in the previous task in order to achieve better results.

First, we tested out our classifier with one extra feature, either the BM25F value, the PageRank score, or the smallest window score. We computed the these three values with our best weights and best choices of functions from PA3. We also tried out other functions for PageRank and smallest window, but they were not as good. On its own, the BM25F score is the most helpful (+0.68), followed by PageRank (+0.48), while smallest window helped just a bit (+0.18).

Then, we tried combinations of two extra features. BM25F in combination with PageRank was by far the most helpful, achieving a score of 0.8660, while the other two combinations were around 0.855. Surprisingly, BM25F and smallest window was not as good as PageRank and smallest window, with shows some combinations are just better than others. Finally, using all three extra features, we got 0.8664 --- only a small rise from BM25F + PageRank.

From all this, it seems that the smallest window score is least useful, while BM25F is best, and PageRank is also good. This could mean that we just didn't come up with a good function to utilize the smallest window score.

Next, we take a look at the ranking output of our system and do some error analysis. Below are a few examples that lead us to design some new features:

For the query \texttt{climbing wall hours}, our system ranked \texttt{http://www.stanford.edu/$\sim$clint/index/} fourth in ten documents. With a relevance score of 0.0, it should have been last.

We took a look at the page, and discovered that it was a personal page of Clint Cummins, about a book he wrote on rock climbing. We realized that the site's url gave away the fact that it was a personal homepage (and therefore less likely to be relevant) because it contained \texttt{$\sim$clint}.

We immediately got the idea to add a binary feature to detect the ``$\sim$" character in urls. After adding the feature, we tested out the system. To our surprise, the performance stayed the same --- the feature didn't work. It took some more analysis for it to dawn on us that having a ``$\sim$" is not necessarily a bad sign --- many queries actually ask for personal homepages for professors, such as the query \texttt{jeffery ullman}, which has the url \texttt{http://infolab.stanford.edu/$\sim$ullman/} at relevance 3.0. This meant that detecting the ``$\sim$" was not enough --- we needed to detect if the character appears in a url while the query does not ask for that person. With this new insight, we came up with an improved feature that is still binary, but is only true if (1), a tilde appears and (2), no query terms appear in the url. With this feature, we improved our system performance by 0.01, which is a good increase. For the specific error case, \texttt{http://www.stanford.edu/$\sim$clint/index/} dropped a couple places, which was encouraging. 

Another error we noticed was for the query \texttt{andrew ng computer science stanford}, where our system ranked a link to New York Times story in the top 3. The story was indeed about Andrew Ng and is a good read, but would probably not fulfill the information need of the person with that query. We noticed that the document url was very long, and contained a lot of characters such as ``\&", ``?" and ``=".

Therefore, we came up with two lines of attack: (1) Add features related to url length. (2) Add features that detect special characters. For url length, we tried number of characters, number of words and number of segments (seperated by slashes). We also tried doing log-scaling for these features. In the end, we came up with the feature $\log{num_words-4.5}$, which seemed to work best. For special characters, we tried adding binary features that detected their existence in the url, but they didn't really help. With the new url length feature, we observed a slightly lower ranking for docs with really long urls (the log scaling meant the feature was only prominent for exceedingly wordy urls), and an overall performance boost.

Finally, we made further observations on the training and test data and felt that the file type could be a good feature to add. (It seemed to be the case that many PDF files got high scores and PPT files had low relevance scores.) After adding two feature to detect PDF and PPT files, our ranking performance went up by 0.018, a considerable increase.

Apart from that, we also imported our extra features from PA3. Some of them worked well, namely ``number of fields where query terms appear'' and ``number of query terms that are missing from document''. We also added the log normalized document body length as a feature, with help a bit.

Finally, we did a grid search for parameters. Similar to task 2, the linear kernel SVM performs well for a large range of $C$ values, while RBF SVM has a sweet spot of approximately $C\in[2^{2},2^{3}]$ and $\gamma\in[2^{-4},2^{-5}]$. Peak performance for both are quite similar. With all features in effect, RBF SVM got the highest score of 0.8750 with grid search, but linear SVM consistently gets over 0.8745 for a large range of parameters.

Finally, our performance for task 3 (numbers are from linear SVM, RBF SVM very similar):
\begin{table}[H] 
\centering
\begin{tabular}{|c|c|c|}
\hline
Features &Train& Dev \\\hline
base & 0.8637 & 0.8479\\\hline
base+bm25 & 0.8782 & 0.8547\\\hline
base+PR & 0.8688 & 0.8527\\\hline
base+win & 0.8668 & 0.8497\\\hline
base+bm25+PR & 0.8820 & 0.8660\\\hline
base+bm25+win & 0.8789 & 0.8541\\\hline
base+PR+win & 0.8722 & 0.8568\\\hline
base+bm25+PR+win & 0.8819 & 0.8664\\\hline
7 extra features & 0.8896 & 0.8747\\\hline
after grid search (RBF)  & 0.8894 & 0.8750\\\hline
\end{tabular}
\end{table}

\section*{Task 4: Extra Credit}
For extra credit, we experimented with using SVM regression for the pointwise approach. Using the new features, our regression SVM achieves a score of 0.8632, which is a large improvement on task 2. Also, our regression SVM performs better than a linear regression model using the same features.

Our performance for task 4:
\begin{table}[H]
\centering
\begin{tabular}{|c|c|}
\hline
Dataset & NDCG/iNDCG score \\\hline
training & 0.8897\\\hline
dev & 0.8632\\\hline
\end{tabular}
\end{table}

\end{document}