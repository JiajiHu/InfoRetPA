\documentclass[10pt,twocolumn]{article}
\usepackage{fullpage,enumerate,amsmath,amssymb,graphicx,setspace,epstopdf,float,multirow}

\begin{document}
\title{CS 276 Programming Assignment 4 Project Report}
\author{Jiaji Hu, Xuening Liu}
\date{}
\maketitle

\section*{Task 1: Pointwise Approach}
In this part, we use linear regression to give a score to each query-document pair. The query-document pairs are represented by a five-dimensional vector of tf-idf scores.

Since we cannot start adding new features at this point, and there are no parameters to tune for linear regression, there is not much for us to design ourselves in this task. Here, we focus on two design choices:

Firstly, we determined whether or not to use sublinear scaling for term frequencies. By using sublinear scaling, we design the system so that high frequencies of a single term have a smaller impact on the score. Conceptually this makes sense, because having a query term appear 10 times is probably better than having it appear 1 time, but it probably shouldn't be 10 times as good. With sublinear scaling, our linear regression model achieved a high score, so we used sublinear scaling.

Secondly, we tried applying length normalization to each document. The idea is that since longer documents are more likely to have more query terms appear, we should penalize them for their length. Furthermore, we can smooth the document lengths for better performance. It turned out that document length normalization did not help performance on this task. However, length normalization did help in the following tasks, as we will discuss later.

Finally, our performance for task 1:
\begin{table}[H]
\centering
\begin{tabular}{|c|c|}
\hline
Dataset & NDCG/iNDCG score \\\hline
training & 0.8749\\\hline
dev & 0.8429\\\hline
\end{tabular}
\end{table}

\section*{Task 2: Pairwise Approach}
The pairwise approach is totally different from the pointwise approach, in that we do not directly try to predict a relevance score for a query-document pair, but train our classifier to compare the relevance of different query-document pairs.

Similar to task 1, sublinear scaling of term frequencies and document length normalization are two things we can do to help the performance of the ranking system. In this task, both sublinear scaling and length normalization gave the NDCG score a considerable boost, so we do both these operations in our final system.

Standardization for the input for the SVM is necessary for fast runtime and good performance. In the assignment guidelines, we were instructed to do standardization \underline{before} we took the difference of vectors. Also, we were instructed to do standardization independently on the training and test set. We felt that these might not be the correct practice, but rather shortcuts to make the assignment simpler. Therefore, we implemented both versions of standardization to see if they lead to any difference in performance.

Firstly, since the actual vectors that get sent into the SVM for training are the vectors after we take the difference, it makes more sense to do standardization on that dataset. Secondly, it is a good idea to do the same preprocessing to training and test data, so we should not do separate standardization for training and test. In our second implementation, we implemented our own standardization method that remembers the mean and standard deviation of the training data, and apply the same filter to the test data as the training data.

In practice, both the standardization recommended in the guidelines and our own standardization give good results. It makes sense that the shortcuts work. For the first shortcut, we know that we don't need the input data to exactly have zero mean and unit variance for SVM to work well. Standardization before taking the difference gives a distribution that is good enough. For the second shortcut, we rely on the fact that the training and test data have very similar distributions, so doing standardization separately yields very similar results as doing the same transformation on the two datasets.

To tune the SVM parameters, we used grid search on parameter \texttt{C} for linear kernel SVM, and parameters $C$ and $\gamma$ for RBF SVM. We searched on the following range: $C=\{2^{-3},2^{-2},\dots,2^{3}\}$, and $\gamma=\{2^{-7},2^{-6},\dots,2^{-1}\}$. We noticed that with large $C$ and large $\gamma$, the training time goes up considerably. Also, the performance of the linear SVM does not vary a lot with $C$, and the liner SVM performs very well even under default parameters. For the RBF SVM, we got some good performance with $C=\{2^{2},2^{3}\}$ and $\gamma=\{2^{-4},2^{-5}\}$, though there is a lot of variance in the results.

Comparing the linear and RBF kernel SVMs, both give good performance. The RBF SVM got a slightly higher top NDCG score after grid search, but not by much. On the other hand, the linear SVM is handy because it is generally faster to train and is not as sensitive to parameter adjustment.

Finally, our performance for task 2:
\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
Dataset & Linear SVM & RBF SVM \\\hline
training & 0.8627 & 0.8637 \\\hline
dev & 0.8472 & 0.8479\\\hline
\end{tabular}
\end{table}

\section*{Task 3: More Features and \\Error Analysis}
In this task, we add extra features to the model in the previous task in order to achieve better results.

First, we tested out our classifier with one extra feature, either the BM25F value, the PageRank score, or the smallest window score. We computed the these three values with our best weights and best choices of functions from PA3. We also tried out other functions for PageRank and smallest window, but they were not as good. On its own, the BM25F score is the most helpful (+0.68), followed by PageRank (+0.48), while smallest window helped just a bit (+0.18).

Then, we tried combinations of two extra features. BM25F in combination with PageRank was by far the most helpful, achieving a score of 0.8660, while the other two combinations were around 0.855. Surprisingly, BM25F and smallest window was not as good as PageRank and smallest window, with shows some combinations are just better than others. Finally, using all three extra features, we got 0.8664 --- only a small rise from BM25F + PageRank.

From all this, it seems that the smallest window score is least useful, while BM25F is best, and PageRank is also good. This could mean that we just didn't come up with a good function to utilize the smallest window score.

Next, we take a look at the ranking output of our system and do error analysis. A sample output is shown below (we show the given relevance scores for each document to highlight errors).
\begin{verbatim}

\end{verbatim}
We see two mistakes in the above ranking. 

Looking at the relevance file, we also came up with other ideas for features:

Finally, we did a grid search for parameters. Similar to task 2, the linear kernel SVM performs well for a large range of $C$ values, while RBF SVM has a sweet spot of approximately $C\in[2^{2},2^{3}]$ and $\gamma\in[2^{-4},2^{-5}]$. Peak performance for both are quite similar.

Finally, our performance for task 3:
\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
Features &Train& Dev \\\hline
base & 0.8637 & 0.8479\\\hline
base+bm25 & 0.8782 & 0.8547\\\hline
base+PR & 0.8688 & 0.8527\\\hline
base+win & 0.8668 & 0.8497\\\hline
base+bm25+PR & 0.8820 & 0.8660\\\hline
base+bm25+win & 0.8789 & 0.8541\\\hline
base+PR+win & 0.8722 & 0.8568\\\hline
base+bm25+PR+win & 0.8819 & 0.8664\\\hline
6 extra features & 0.8888 & 0.8737\\\hline
after grid search  & 0.8873 & 0.8743\\\hline
\end{tabular}
\end{table}

\section*{Task 4: Extra Credit}
For extra credit, we experimented with using SVM regression for the pointwise approach. Using the new features, our regression SVM achieves a score of 0.8632, which is a large improvement on task 2. Also, our regression SVM performs better than a linear regression model using the same features.

Our performance for task 4:
\begin{table}[H]
\centering
\begin{tabular}{|c|c|}
\hline
Dataset & NDCG/iNDCG score \\\hline
training & 0.8897\\\hline
dev & 0.8632\\\hline
\end{tabular}
\end{table}

\end{document}