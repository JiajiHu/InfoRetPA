\documentclass[10pt,twocolumn]{article}
\usepackage{fullpage,enumerate,amsmath,amssymb,graphicx,setspace,epstopdf,float,multirow}

\begin{document}
\title{CS 276 Programming Assignment 3 Project Report}
\author{Jiaji Hu, Xuening Liu}
\date{}
\maketitle

\section*{Task 1: Cosine Similarity}
%Our parameters for task 1:
%\begin{table}[H]
%\centering
%\begin{tabular}{|c|c|}
%\hline
%Parameter & value \\\hline
%task1\_W\_url & 3.1\\\hline
%task1\_W\_title & 5.0\\\hline
%task1\_W\_body & 1.1\\\hline
%task1\_W\_header & 1.6\\\hline
%task1\_W\_anchor & 0.5\\\hline
%body\_len\_normalization & 500\\\hline
%tf\_scaling & linear\\\hline
%\end{tabular}
%\end{table}
After extensive tuning, we figured out that tuning parameters for cosine similarity does rather little to help with performance. However, we used this task to develop our methodology for tuning parameters.

First of all, we note that if we just tuned one parameter, with the others the same, there was often one peak value for that parameter, e.g. below:
\begin{figure}[H]
\centering
\includegraphics[width=0.6\linewidth]{fig1}
\end{figure}
It was usually the case that there is only one such peak value, and that we were able to find it by simple ``hill climbing''.

After we found the peak for one parameter, we could move on to the next parameter and find its peak value. By making a few iterations through all the parameters, we were able to more or less converge to a set a parameters that gave good performance.

The advantage to such a method is that it is simple, effective and easy to understand. It was often the case that we were able to converge to a local maximum in a few dozen tries. The results were also quite good, especially for our BM25F scorer, which ended up getting huge improvements.

The drawback of hill climbing is that it may get stuck in local optima. We observed that since there is an ordering of which parameters get tuned, the first ones tend to overfit the parameters yet to be tuned. When the time comes to tune the last parameters, little adjustment needs to be made, because the algorithm is already stuck at a local optimum. To combat this, we used randomization and repetition.

Finally, some discussion on the reasoning behind our final parameter weights for cosine similarity, and some attempts to explain the results:

First, the choice to use linear or sublinear scaling for term frequencies: Using sublinear scaling will greatly lower the impact of high frequency appearances of a query term in a field. Note that this would mostly affect the \texttt{body} and \texttt{anchor} fields. On a high level, this is a design choice of how much emphasis we want to put on the fact that a query term appears many times in the same field. In practice, linear scaling gave better performance on the cosine similarity scorer, so that was our choice.

Next, we discuss the weights for the five fields. Intuitively, higher weights should be given to fields that are good predictors of relevance. In practice, we realized that because we were not doing good normalization, we also needed to take into account the fact that some fields were more likely to gain higher counts of the query terms than others. The following is a list of the field weights and some explanation.
\begin{enumerate}[(a)]
\item \texttt{task1\_W\_url} : 3.1

The high weight for URL may be because query terms don't always appear in the URL, but when they do, they are great indicators of the document's relevance.
\item \texttt{task1\_W\_title} : 5.0

As expected, \texttt{Title} got the highest weight. It follows intuition that having the query appear in the page title indicates high relevance. 
\item \texttt{task1\_W\_body} : 1.1

Note that the \texttt{body} field often contains a high number of occurrences of query terms. Since we are not using sublinear scaling, this has a even larger impact on the document's score. To prevent the influence from overshadowing input from the other fields, the weight for the body field cannot be too high.
\item \texttt{task1\_W\_header} : 1.6

We note that there are other sets of high-performing parameters that have the weight for this field to be higher than 1.6. We concluded that the header field is not such a high-impact field as \texttt{title}, due to its content being not as indicative.
\item \texttt{task1\_W\_anchor} : 0.5

We were surprised to find such a low weight. It might be because the anchor field often has a high number of query term occurrences, so the weights need to be lower to normalize for that.
\end{enumerate}

Lastly, we discuss body length smoothing. This directly affects the score, as more smoothing would mean less harsh normalization for longer documents compared to the shorter ones. From our experiments, the best value was found to be 500.

Finally, our performance for task 1:
\begin{table}[H]
\centering
\begin{tabular}{|c|c|}
\hline
Dataset & NDCG/iNDCG score \\\hline
training & 0.8671\\\hline
dev & 0.8462\\\hline
\end{tabular}
\end{table}

\section*{Task 2: BM25F}
%Our parameters for task 2:
%\begin{table}[H]
%\centering
%\begin{tabular}{|c|c|c|}
%\hline
%Field & W param & B param\\\hline
%url & 3.3 & 0.0\\\hline
%title & 5.2 & 0.2\\\hline
%body & 0.9 & 0.8\\\hline
%header & 2.85 & 0.5 \\\hline
%anchor & 3.45 & 0.0\\\hline
%\end{tabular}
%\end{table}
%\begin{table}[H]
%\centering
%\begin{tabular}{|c|c|}
%\hline
%Parameter & value\\\hline
%tf\_scaling & sublinear\\\hline
%K1 & 4.9\\\hline
%$\lambda$ & 3.25\\\hline
%$\lambda'$ & 0.05\\\hline
%$\lambda''$ & 0.1\\\hline
%\end{tabular}
%\end{table}
%Where the function $V_j$ is as follows:
%\begin{equation*}
%V_j = \frac{1}{\lambda'+\exp({-f_j\lambda''})}
%\end{equation*}
Our approach for tuning is the same as in task 1, with one difference: In this task, we note that \texttt{W} and \texttt{B} are best tuned as a couple. This makes sense as \texttt{B} influences the field score and \texttt{W} decides its weight. 

For task 2, we chose to use sublinear term frequency scaling, based on the results we got from parameter tuning. As for the parameter weights, they are discussed below:
\begin{enumerate}[(a)]
\item \texttt{task2\_W\_url}:3.3, \texttt{task2\_B\_url}:0.0

\item \texttt{task2\_W\_title}:5.2, \texttt{task2\_B\_title}:0.2

\item \texttt{task2\_W\_body}:0.9, \texttt{task2\_B\_body}:0.8

\item \texttt{task2\_W\_header}:2.85, \texttt{task2\_B\_header}:0.5

\item \texttt{task2\_W\_anchor}:3.45, \texttt{task2\_B\_anchor}:0.0

\item \texttt{K1} : 4.9
\item $V_j = \frac{1}{\lambda'+\exp({-f_j\lambda''})}$, \texttt{$\lambda$} : 3.25, \texttt{$\lambda'$} : 0.05, \texttt{$\lambda''$} : 0.1

\end{enumerate}

Lastly, 


NOTE: give values for weights and discuss intuition.

What was the reasoning behind giving the weights to the url, title,
body, header and anchor fields for the three tasks? Were there any
particular properties about the documents that allowed a higher weight
to be given to one field as opposed to another?

In BM25F, in addition to the weights given to the fields, there are
8 other parameters, Burl;Btitle;Bheader;Bbody;Banchor, lambda; lambda\_prime and K1.
How do these parameters affect the ranking function?

In BM25F, why did you select a particular Vj function?

And our performance on the datasets:
\begin{table}[H]
\centering
\begin{tabular}{|c|c|}
\hline
Dataset & NDCG/iNDCG score \\\hline
training & 0.8883\\\hline
dev & 0.8689\\\hline
\end{tabular}
\end{table}
\section*{Task 3: Smallest Window}
In this task, we add a boosting function onto the BM25F scorer. It turned out that the best set of parameters for BM25F and the best set for smallest window was very similar. This makes sense because we still want the BM25 scorer to do its best -- we are just adding a modification to its best effort score. 

Therefore, we will not repeat our explanations for the weights that appeared in task 2. Instead, we will list them in the table below, and go on to focus on the boost function and the value of \texttt{B}.
\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
Field & W param & B param\\\hline
url & 3.3 & 0.0\\\hline
title & 5.2 & 0.2\\\hline
body & 0.9 & 0.8\\\hline
header & 2.85 & 0.5 \\\hline
anchor & 3.45 & 0.0\\\hline
\end{tabular}
\end{table}
\begin{table}[H]
\centering
\begin{tabular}{|c|c|}
\hline
Parameter & value\\\hline
tf\_scaling & sublinear\\\hline
K1 & 4.9\\\hline
$\lambda$ & 3.25\\\hline
$\lambda'$ & 0.05\\\hline
$\lambda''$ & 0.1\\\hline
B & 1.12\\\hline
\end{tabular}
\end{table}
Where $V_j$ is still:
\begin{equation*}
V_j = \frac{1}{\lambda'+\exp({-f_j\lambda''})}
\end{equation*}
In the end, we opted for a function that decreases very rapidly with the increase in window size.
\begin{equation*}
score = score \times [1+(B-1)\exp(query\_len-win\_size)]
\end{equation*}


For a function that includes the smallest window as one component,
how does varying B and the boost function change the performance
of the ranking algorithm?

Finally, our performance on the datasets:
\begin{table}[H]
\centering
\begin{tabular}{|c|c|}
\hline
Dataset & NDCG/iNDCG score \\\hline
training & 0.8900\\\hline
dev & 0.8695\\\hline
\end{tabular}
\end{table}

\section*{Task 4: Extra Credit}
For extra credit, we tried many ideas to add to the smallest window scorer.
\begin{enumerate}[(a)]
\item
\item
\item
\item

\end{enumerate}
\begin{table}[H]
\centering
\begin{tabular}{|c|c|}
\hline
Dataset & NDCG/iNDCG score \\\hline
training & 0.8908\\\hline
dev & 0.8718\\\hline
\end{tabular}
\end{table}

\section*{Summary Analysis}
NOTE: compare ranking functions

What other metrics, not used in this assignment, could be used to
get a better scoring function from the document? The metrics could
either be static (query-independent, e.g. document length) or dynamic
(query-dependent, e.g. smallest window).


\end{document}